{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.optim import Adam, Adagrad, Optimizer\n",
    "from tqdm import trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from triton.profiler import activate\n",
    "\n",
    "from src.recommender_model import RecommenderModel\n",
    "from src.utils import train_model, write_submission, plot_losses, evaluate_model"
   ],
   "id": "865b0f499514a878"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Matrix Factorization - MSE\n",
    "This notebook provides implementations for Matrix Factorization using a simple Mean-Squared Error loss."
   ],
   "id": "f6fc3798601718f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Machine Learning can be used to learn the users and items embeddings. The simplest method uses Mean Squared Error loss to optimize the users and items embeddings.",
   "id": "be6b007ec8e33f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MFModel(nn.Module):\n",
    "\tdef __init__(self, num_users: int, num_items: int, embedding_dim: int, dropout: float, activation = nn.ReLU()):\n",
    "\t\tsuper(MFModel, self).__init__()\n",
    "\n",
    "\t\tself.num_users = num_users\n",
    "\t\tself.num_items = num_items\n",
    "\t\tself.embedding_dim = embedding_dim\n",
    "\n",
    "\t\tself.users_embeddings = nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_dim)\n",
    "\t\tself.items_embeddings = nn.Embedding(num_embeddings=num_items, embedding_dim=embedding_dim)\n",
    "\n",
    "\t\tnn.init.xavier_uniform_(self.users_embeddings.weight)\n",
    "\t\tnn.init.xavier_uniform_(self.items_embeddings.weight)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t\t# self.fc = nn.Linear(in_features=embedding_dim, out_features=1)\n",
    "\t\tself.fc = lambda x: torch.sum(x, dim=-1)  # for standard scalar product\n",
    "\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, users: torch.tensor, items: torch.tensor) -> torch.tensor:\n",
    "\t\tuser_embeddings = self.dropout(self.users_embeddings(users))\n",
    "\t\titem_embeddings = self.dropout(self.items_embeddings(items))\n",
    "\t\treturn self.activation(self.fc(user_embeddings * item_embeddings))"
   ],
   "id": "c236ba2c60863ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class URMDataset(Dataset):\n",
    "\tdef __init__(self, urm: sp.csr_matrix):\n",
    "\t\tself.n: int = urm.nnz\n",
    "\n",
    "\t\turm_coo = urm.tocoo()\n",
    "\t\tself.user_item_coordinates = torch.from_numpy(\n",
    "\t\t\tnp.vstack((urm_coo.row, urm_coo.col)).T\n",
    "\t\t)\n",
    "\t\tself.ratings = torch.from_numpy(\n",
    "\t\t\turm_coo.data\n",
    "\t\t)\n",
    "\n",
    "\tdef __getitem__(self, idx) -> tuple[int, int, float]:\n",
    "\t\tinteraction_coordinates = self.user_item_coordinates[idx]\n",
    "\t\treturn interaction_coordinates[0], interaction_coordinates[1], self.ratings[idx]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n"
   ],
   "id": "2f9a466de9f84b5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MFMSE(RecommenderModel):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MFMSE, self).__init__()\n",
    "\t\tself.mf_model: MFModel | None = None\n",
    "\t\tself.optimizer: Optimizer | None = None\n",
    "\t\tself.loss_fn = None\n",
    "\t\tself.best_map = 0.0\n",
    "\t\tself.checkpoint_dir = Path(\"model_checkpoints\")\n",
    "\t\tself.checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\tdef save_checkpoint(self, map_score: float, epoch: int) -> None:\n",
    "\t\t\"\"\"Save model checkpoint if current MAP@10 is better than previous best\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tmap_score: Current MAP@10 score\n",
    "\t\t\tepoch: Current epoch number\n",
    "\t\t\"\"\"\n",
    "\t\tif map_score > self.best_map:\n",
    "\t\t\tself.best_map = map_score\n",
    "\t\t\tcheckpoint = {\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'model_state_dict': self.mf_model.state_dict(),\n",
    "\t\t\t\t'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "\t\t\t\t'map_score': map_score,\n",
    "\t\t\t\t'embedding_dim': self.mf_model.embedding_dim,\n",
    "\t\t\t\t'num_users': self.mf_model.num_users,\n",
    "\t\t\t\t'num_items': self.mf_model.num_items,\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tcheckpoint_path = self.checkpoint_dir / 'best_model.pth'\n",
    "\t\t\ttorch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\tdef load_best_model(self) -> None:\n",
    "\t\t\"\"\"Load the best performing model\"\"\"\n",
    "\t\tcheckpoint_path = self.checkpoint_dir / 'best_model.pth'\n",
    "\t\tif checkpoint_path.exists():\n",
    "\t\t\tcheckpoint = torch.load(checkpoint_path)\n",
    "\n",
    "\t\t\t# Recreate the model if it doesn't exist\n",
    "\t\t\tif self.mf_model is None:\n",
    "\t\t\t\tself.mf_model = MFModel(\n",
    "\t\t\t\t\tnum_users=checkpoint['num_users'],\n",
    "\t\t\t\t\tnum_items=checkpoint['num_items'],\n",
    "\t\t\t\t\tembedding_dim=checkpoint['embedding_dim'],\n",
    "\t\t\t\t\tdropout=0.0  # We typically don't need dropout during inference\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tself.mf_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\t\t\tif self.optimizer is not None:\n",
    "\t\t\t\tself.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\t\t\tself.best_map = checkpoint['map_score']\n",
    "\t\t\tprint(f\"Loaded best model with MAP@10: {self.best_map:.5f} from epoch {checkpoint['epoch']}\")\n",
    "\t\telse:\n",
    "\t\t\tprint(\"No checkpoint found.\")\n",
    "\n",
    "\tdef fit(self, urm: sp.csr_matrix, icm: sp.csr_matrix, urm_val: sp.csr_matrix, lr: float = .001, embedding_dim: int = 128, dropout: float = .2, epochs: int = 10, batch_size: int = 8192, weight_decay: float = 1e-4, loss_fn=nn.MSELoss(), activation=nn.ReLU(), plot_loss: bool = True)-> None:\n",
    "\t\tself.urm = urm\n",
    "\t\tself.mf_model = MFModel(urm.shape[0], urm.shape[1], embedding_dim, dropout, activation)\n",
    "\t\tself.optimizer = Adam(self.mf_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\t\tself.loss_fn = loss_fn\n",
    "\n",
    "\t\tvalidation_enabled = urm_val.nnz > 0\n",
    "\n",
    "\t\tdataloader = DataLoader(\n",
    "\t\t\tURMDataset(urm),\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tnum_workers=8,\n",
    "\t\t)\n",
    "\t\tdataloader_val = DataLoader(\n",
    "\t\t\tURMDataset(urm_val),\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tnum_workers=8,\n",
    "\t\t)\n",
    "\t\tdl_len = len(dataloader)\n",
    "\n",
    "\t\tloss_history_val = np.zeros(epochs + 1)\n",
    "\t\tmap_history = np.zeros(epochs + 1)\n",
    "\t\tloss_history = np.zeros((dl_len * epochs,))\n",
    "\n",
    "\t\tif validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\tmap_history[0], loss_history_val[0] = self._validate(dataloader_val, urm_val)\n",
    "\t\t\tself.save_checkpoint(map_history[0], 0)\n",
    "\n",
    "\t\tfor epoch in (t := trange(epochs)):\n",
    "\t\t\tself.mf_model.train()\n",
    "\t\t\tfor batch_idx, (users, items, labels) in enumerate(dataloader):\n",
    "\t\t\t\tpred = self.mf_model(\n",
    "\t\t\t\t\tusers,\n",
    "\t\t\t\t\titems\n",
    "\t\t\t\t).squeeze()\n",
    "\t\t\t\tloss = self.loss_fn(pred, labels)\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\t\tloss_history[dl_len * epoch + batch_idx] = loss.item()\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tt.set_postfix({\n",
    "\t\t\t\t\t\t\"Batch progression\": f\"{(batch_idx + 1) / dl_len * 100:.2f}%\",\n",
    "\t\t\t\t\t\t\"Train loss\": f\"{loss.item():.5f}\",\n",
    "\t\t\t\t\t\t\"Val loss\": f\"{loss_history_val[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"MAP@10\": f\"{map_history[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"Best MAP@10\": f\"{self.best_map:.5f}\",\n",
    "\t\t\t\t\t\t\"Pred stats\": f\"(mean:{pred.mean().item():.4f},std:{pred.std().item():.4f})\",\n",
    "\t\t\t\t\t})\n",
    "\t\t\tif validation_enabled:\n",
    "\t\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\t\tmap_history[epoch + 1], loss_history_val[epoch + 1] = self._validate(dataloader_val, urm_val)\n",
    "\t\t\t\tself.save_checkpoint(map_history[epoch + 1], epoch + 1)\n",
    "\n",
    "\t\tif not validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()  # as it has not been done before\n",
    "\t\tplot_losses(epochs, loss_history, loss_history_val, len(dataloader), ('MAP@10', [x * len(dataloader) for x in range(epochs + 1)], map_history))\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _compute_full_urm_pred(self, use_linear: bool = False) -> None:\n",
    "\t\t\"\"\"In-place computation of the final predicted URM matrix using the final Linear layer\"\"\"\n",
    "\t\tself.mf_model.eval()\n",
    "\t\tdel self.urm_pred  # free memory\n",
    "\n",
    "\t\tif use_linear:\n",
    "\t\t\tw = self.mf_model.fc.weight\n",
    "\t\t\tw_sign = torch.sign(w)\n",
    "\t\t\tw_sqrt = torch.sqrt(torch.abs(w))\n",
    "\t\t\tb = self.mf_model.fc.bias.item() or 0\n",
    "\n",
    "\t\t\tweighted_users_embeddings = self.mf_model.users_embeddings.weight * w_sqrt * w_sign\n",
    "\t\t\tweighted_items_embeddings = self.mf_model.items_embeddings.weight * w_sqrt\n",
    "\n",
    "\t\t\tself.urm_pred = self.mf_model.activation(\n",
    "\t\t\t\tweighted_users_embeddings @ weighted_items_embeddings.T + b\n",
    "\t\t\t).numpy()\n",
    "\t\telse:\n",
    "\t\t\tself.urm_pred = (\n",
    "\t\t\t\tself.mf_model.users_embeddings.weight @ self.mf_model.items_embeddings.weight.T\n",
    "\t\t\t).numpy()\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _validate(self, dataloader_val: DataLoader, urm_val: sp.csr_matrix) -> tuple[float, float]:\n",
    "\t\t\"\"\"Returns the MAP@10 and the validation loss\"\"\"\n",
    "\t\tself.mf_model.eval()\n",
    "\t\tloss = 0\n",
    "\t\tfor users, items, labels in dataloader_val:\n",
    "\t\t\tpred = self.mf_model(\n",
    "\t\t\t\tusers,\n",
    "\t\t\t\titems\n",
    "\t\t\t).squeeze()\n",
    "\t\t\tloss += self.loss_fn(pred, labels)\n",
    "\n",
    "\t\tloss /= len(dataloader_val)\n",
    "\t\treturn evaluate_model(self, urm_val, users_to_test=.1), loss.item()"
   ],
   "id": "37d17a7915bec271"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "mfmse_train = train_model(MFMSE(), batch_size=8192, epochs=100, embedding_dim=16, loss_fn=nn.MSELoss(), activation=nn.ReLU())",
   "id": "2f57c8af94d5c95e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We see that the MSE loss is not effective and leads to poor performance in terms of MAP@10.",
   "id": "7df56d227e416dce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mfmse_submission = train_model(MFMSE(), test_size=0, batch_size=8192, epochs=15, embedding_dim=16, loss_fn=nn.MSELoss(), activation=nn.ReLU())\n",
    "write_submission(mfmse_submission, \"mf_mse_submission.csv\")"
   ],
   "id": "b15db8626b91a0ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Submission result: `0.00278`",
   "id": "d1c2fbba9c4d71f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
