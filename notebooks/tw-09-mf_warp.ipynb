{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.optim import Adam, Optimizer\n",
    "from tqdm import trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from src.recommender_model import RecommenderModel\n",
    "from src.utils import train_model, write_submission, plot_losses, evaluate_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Matrix Factorization - WARP\n",
    "The Bayesian Personalized Ranking loss using the WARP sampling could be more effective."
   ],
   "id": "2c5640c32faeaee4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MFModel(nn.Module):\n",
    "\tdef __init__(self, num_users: int, num_items: int, embedding_dim: int, dropout: float = .2, activation = nn.ReLU(), use_linear: bool = False):\n",
    "\t\tsuper(MFModel, self).__init__()\n",
    "\n",
    "\t\tself.num_users = num_users\n",
    "\t\tself.num_items = num_items\n",
    "\t\tself.embedding_dim = embedding_dim\n",
    "\n",
    "\t\tself.users_embeddings = nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_dim)\n",
    "\t\tself.items_embeddings = nn.Embedding(num_embeddings=num_items, embedding_dim=embedding_dim)\n",
    "\n",
    "\t\tnn.init.xavier_uniform_(self.users_embeddings.weight, gain=1.0)\n",
    "\t\tnn.init.xavier_uniform_(self.items_embeddings.weight, gain=1.0)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t\tif use_linear:\n",
    "\t\t\tself.fc = nn.Linear(in_features=embedding_dim, out_features=1)\n",
    "\t\telse:\n",
    "\t\t\tself.fc = lambda x: torch.sum(x, dim=-1, keepdim=True)  # for standard scalar product\n",
    "\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, users: torch.tensor, items: torch.tensor) -> torch.tensor:\n",
    "\t\tuser_embeddings = self.dropout(self.users_embeddings(users))\n",
    "\t\titem_embeddings = self.dropout(self.items_embeddings(items))\n",
    "\t\treturn self.activation(self.fc(user_embeddings * item_embeddings))"
   ],
   "id": "c7fa3ccc875b11ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class URMDatasetWARP(Dataset):\n",
    "\t\"\"\"URM Dataset with negative sampling\"\"\"\n",
    "\tdef __init__(self, urm: sp.csr_matrix, model_ref, margin = 0, max_attempts = 5):\n",
    "\t\tself.urm: sp.csr_matrix = urm\n",
    "\t\tself.num_items: int = self.urm.shape[1]\n",
    "\n",
    "\t\turm_coo = self.urm.tocoo()\n",
    "\t\tself.user_item_coordinates = torch.from_numpy(\n",
    "\t\t\tnp.vstack((urm_coo.row, urm_coo.col)).T\n",
    "\t\t)\n",
    "\t\t# self.ratings = None  # assume implicit ratings\n",
    "\t\tself.user_item_sets = {\n",
    "\t\t\tuser_id: set(\n",
    "\t\t\t\tself.urm.getrow(user_id).indices\n",
    "\t\t\t)\n",
    "\t\t\tfor user_id in range(self.urm.shape[0])\n",
    "\t\t}  # we use sets to check if an item has been interacted with as it has constant access time\n",
    "\n",
    "\t\tself.model_ref = model_ref  # reference to the model object which is used to sample negative items\n",
    "\t\tself.margin = margin\n",
    "\t\tself.max_attempts = max_attempts\n",
    "\n",
    "\tdef sample_negative_item(self, user, pos_sample):\n",
    "\t\tviolates_ranking, neg_sample, attempts = False, 0, 0\n",
    "\t\twhile attempts < self.max_attempts and not violates_ranking:\n",
    "\t\t\tneg_sample = torch.randint(self.num_items, size=())\n",
    "\t\t\twhile neg_sample in self.user_item_sets[user.item()]:\n",
    "\t\t\t\tneg_sample = torch.randint(self.num_items, size=())\n",
    "\n",
    "\t\t\tif self.model_ref(user, pos_sample).item() + self.margin < self.model_ref(user, neg_sample).item():\n",
    "\t\t\t\tviolates_ranking = True\n",
    "\n",
    "\t\t\tattempts += 1\n",
    "\n",
    "\t\treturn neg_sample\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tuser, pos_sample = self.user_item_coordinates[idx]\n",
    "\t\treturn user, pos_sample, self.sample_negative_item(user, pos_sample)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.urm.nnz"
   ],
   "id": "99fc72e205f59d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bpr_loss(pos_scores, neg_scores):\n",
    "\treturn -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores)))"
   ],
   "id": "4f36eba9f0419db6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MFBPR(RecommenderModel):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MFBPR, self).__init__()\n",
    "\t\tself.mf_model: MFModel | None = None\n",
    "\t\tself.optimizer: Optimizer | None = None\n",
    "\t\tself.loss_fn = None\n",
    "\t\tself.use_linear: bool = False\n",
    "\t\tself.best_map = 0.0\n",
    "\t\tself.checkpoint_dir = Path(\"model_checkpoints\")\n",
    "\t\tself.checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\tdef save_checkpoint(self, map_score: float, epoch: int) -> None:\n",
    "\t\t\"\"\"Save model checkpoint if current MAP@10 is better than previous best\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tmap_score: Current MAP@10 score\n",
    "\t\t\tepoch: Current epoch number\n",
    "\t\t\"\"\"\n",
    "\t\tif map_score > self.best_map:\n",
    "\t\t\tself.best_map = map_score\n",
    "\t\t\tcheckpoint = {\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'model_state_dict': self.mf_model.state_dict(),\n",
    "\t\t\t\t'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "\t\t\t\t'map_score': map_score,\n",
    "\t\t\t\t'embedding_dim': self.mf_model.embedding_dim,\n",
    "\t\t\t\t'num_users': self.mf_model.num_users,\n",
    "\t\t\t\t'num_items': self.mf_model.num_items,\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tcheckpoint_path = self.checkpoint_dir / 'best_model.pth'\n",
    "\t\t\ttorch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\tdef load_best_model(self) -> None:\n",
    "\t\t\"\"\"Load the best performing model\"\"\"\n",
    "\t\tcheckpoint_path = self.checkpoint_dir / 'best_model.pth'\n",
    "\t\tif checkpoint_path.exists():\n",
    "\t\t\tcheckpoint = torch.load(checkpoint_path)\n",
    "\n",
    "\t\t\t# Recreate the model if it doesn't exist\n",
    "\t\t\tif self.mf_model is None:\n",
    "\t\t\t\tself.mf_model = MFModel(\n",
    "\t\t\t\t\tnum_users=checkpoint['num_users'],\n",
    "\t\t\t\t\tnum_items=checkpoint['num_items'],\n",
    "\t\t\t\t\tembedding_dim=checkpoint['embedding_dim'],\n",
    "\t\t\t\t\tdropout=0.0  # We typically don't need dropout during inference\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tself.mf_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\t\t\tif self.optimizer is not None:\n",
    "\t\t\t\tself.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\t\t\tself.best_map = checkpoint['map_score']\n",
    "\t\t\tprint(f\"Loaded best model with MAP@10: {self.best_map:.5f} from epoch {checkpoint['epoch']}\")\n",
    "\t\telse:\n",
    "\t\t\tprint(\"No checkpoint found.\")\n",
    "\n",
    "\tdef fit(self, urm: sp.csr_matrix, icm: sp.csr_matrix, urm_val: sp.csr_matrix, lr: float = .001, embedding_dim: int = 128, dropout: float = .2, epochs: int = 10, batch_size: int = 8192, weight_decay: float = 1e-6, loss_fn=bpr_loss, activation=nn.ReLU(), use_linear: bool = False, max_attempts: int = 5, margin: float = 0., plot_loss: bool = True)-> None:\n",
    "\t\tself.urm = urm\n",
    "\t\tself.mf_model = MFModel(urm.shape[0], urm.shape[1], embedding_dim, dropout, use_linear=use_linear)\n",
    "\t\tself.optimizer = Adam(self.mf_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\t\tself.loss_fn = loss_fn\n",
    "\t\tself.use_linear = use_linear\n",
    "\n",
    "\t\tvalidation_enabled = urm_val.nnz > 0\n",
    "\n",
    "\t\tprint(\"Building the datasets...\")\n",
    "\t\tdataloader = DataLoader(\n",
    "\t\t\tURMDatasetWARP(urm, self.mf_model, margin, max_attempts),\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tnum_workers=8,\n",
    "\t\t)\n",
    "\t\tdataloader_val = DataLoader(\n",
    "\t\t\tURMDatasetWARP(urm_val, self.mf_model, margin, max_attempts),\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tnum_workers=8,\n",
    "\t\t)\n",
    "\t\tdl_len = len(dataloader)\n",
    "\n",
    "\t\tloss_history_val = np.zeros(epochs + 1)\n",
    "\t\tmap_history = np.zeros(epochs + 1)\n",
    "\t\tloss_history = np.zeros((dl_len * epochs,))\n",
    "\n",
    "\t\tif validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\tmap_history[0], loss_history_val[0] = self._validate(dataloader_val, urm_val)\n",
    "\t\t\tself.save_checkpoint(map_history[0], 0)\n",
    "\n",
    "\t\tprint(\"Training the model...\")\n",
    "\t\tfor epoch in (t := trange(epochs)):\n",
    "\t\t\tself.mf_model.train()\n",
    "\t\t\tfor batch_idx, (users, pos_samples, neg_samples) in enumerate(dataloader):\n",
    "\t\t\t\tpos_scores = self.mf_model(\n",
    "\t\t\t\t\tusers,\n",
    "\t\t\t\t\tpos_samples\n",
    "\t\t\t\t).squeeze()\n",
    "\t\t\t\tneg_scores = self.mf_model(\n",
    "\t\t\t\t\tusers,\n",
    "\t\t\t\t\tneg_samples\n",
    "\t\t\t\t).squeeze()\n",
    "\t\t\t\tloss = self.loss_fn(pos_scores, neg_scores)\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\t\tloss_history[dl_len * epoch + batch_idx] = loss.item()\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tt.set_postfix({\n",
    "\t\t\t\t\t\t\"Batch\": f\"{(batch_idx + 1) / dl_len * 100:.2f}%\",\n",
    "\t\t\t\t\t\t\"Train loss\": f\"{loss.item():.5f}\",\n",
    "\t\t\t\t\t\t\"Val loss\": f\"{loss_history_val[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"MAP@10\": f\"{map_history[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"Best MAP@10\": f\"{self.best_map:.5f}\",\n",
    "\t\t\t\t\t\t\"Pred stats\": f\"(pos_mean:{pos_scores.mean().item():.4f}, neg_mean:{neg_scores.mean().item():.4f})\",\n",
    "\t\t\t\t\t})\n",
    "\t\t\tif validation_enabled:\n",
    "\t\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\t\tmap_history[epoch + 1], loss_history_val[epoch + 1] = self._validate(dataloader_val, urm_val)\n",
    "\t\t\t\tself.save_checkpoint(map_history[epoch + 1], epoch + 1)\n",
    "\n",
    "\t\tif not validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()  # as it has not been done before\n",
    "\n",
    "\t\tplot_losses(epochs, loss_history, loss_history_val, len(dataloader), ('MAP@10', [x * len(dataloader) for x in range(epochs + 1)], map_history))\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _compute_full_urm_pred(self) -> None:\n",
    "\t\t\"\"\"In-place computation of the final predicted URM matrix using the final Linear layer\"\"\"\n",
    "\t\tself.mf_model.eval()\n",
    "\t\tdel self.urm_pred  # free memory\n",
    "\n",
    "\t\tif self.use_linear:\n",
    "\t\t\tw = self.mf_model.fc.weight\n",
    "\t\t\tw_sign = torch.sign(w)\n",
    "\t\t\tw_sqrt = torch.sqrt(torch.abs(w))\n",
    "\t\t\tb = self.mf_model.fc.bias.item() or 0\n",
    "\n",
    "\t\t\tweighted_users_embeddings = self.mf_model.users_embeddings.weight * w_sqrt * w_sign\n",
    "\t\t\tweighted_items_embeddings = self.mf_model.items_embeddings.weight * w_sqrt\n",
    "\n",
    "\t\t\tself.urm_pred = self.mf_model.activation(\n",
    "\t\t\t\tweighted_users_embeddings @ weighted_items_embeddings.T + b\n",
    "\t\t\t).numpy()\n",
    "\t\telse:\n",
    "\t\t\tself.urm_pred = (\n",
    "\t\t\t\tself.mf_model.users_embeddings.weight @ self.mf_model.items_embeddings.weight.T\n",
    "\t\t\t).numpy()\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _validate(self, dataloader_val: DataLoader, urm_val: sp.csr_matrix) -> tuple[float, float]:\n",
    "\t\tself.mf_model.eval()\n",
    "\t\tloss = 0\n",
    "\t\tfor users, pos_samples, neg_samples in dataloader_val:\n",
    "\t\t\tpos_scores = self.mf_model(\n",
    "\t\t\t\tusers,\n",
    "\t\t\t\tpos_samples\n",
    "\t\t\t).squeeze()\n",
    "\t\t\tneg_scores = self.mf_model(\n",
    "\t\t\t\tusers,\n",
    "\t\t\t\tneg_samples\n",
    "\t\t\t).squeeze()\n",
    "\t\t\tloss += self.loss_fn(pos_scores, neg_scores)\n",
    "\n",
    "\t\tloss /= len(dataloader_val)\n",
    "\t\treturn evaluate_model(self, urm_val, users_to_test=.2), loss.item()"
   ],
   "id": "df6f3c7f6fbb11d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "bpr_train = train_model(MFBPR(), batch_size=8192, epochs=20, embedding_dim=16, weight_decay=1e-7, loss_fn=bpr_loss, activation=nn.Sigmoid(), use_linear=True)",
   "id": "6212e3e23ecfde25",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
