{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-23T10:46:46.442503Z",
     "start_time": "2024-11-23T10:46:43.183796Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import sparse as sp\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Optimizer, Adam\n",
    "from tqdm import trange\n",
    "\n",
    "from src.recommender_model import RecommenderModel\n",
    "from src.utils import train_model, evaluate_model, plot_losses"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MultVAE\n",
    "This notebook provides an implementation of the MultVAE model. See [Variational Autoencoders for Collaborative Filtering, Dawen Liang](https://arxiv.org/pdf/1802.05814)."
   ],
   "id": "ea636d1acb2fbcae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T10:46:46.471622Z",
     "start_time": "2024-11-23T10:46:46.457575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VAE(nn.Module):\n",
    "\tdef __init__(self, input_dim: int, hidden_dims: list[int] = None, latent_dim: int = 256, dropout: float = .5):\n",
    "\t\tsuper(VAE, self).__init__()\n",
    "\n",
    "\t\tactivation = nn.Tanh()\n",
    "\t\tself.latent_dim = latent_dim\n",
    "\n",
    "\t\tif hidden_dims is None:\n",
    "\t\t\thidden_dims = [1024]\n",
    "\n",
    "\t\tencoder_layers = []\n",
    "\t\tprev_dim = input_dim\n",
    "\t\tfor hidden_dim in hidden_dims:\n",
    "\t\t\tencoder_layers.extend([\n",
    "\t\t\t\tnn.Linear(prev_dim, hidden_dim),\n",
    "\t\t\t\tactivation,\n",
    "\t\t\t])\n",
    "\t\t\tprev_dim = hidden_dim\n",
    "\t\tself.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "\t\tself.distribution_parameters = nn.Linear(hidden_dims[-1], 2 * latent_dim)\n",
    "\n",
    "\t\tdecoder_layers = []\n",
    "\t\tprev_dim = latent_dim\n",
    "\t\tfor hidden_dim in reversed(hidden_dims):\n",
    "\t\t\tdecoder_layers.extend([\n",
    "\t\t\t\tnn.Linear(prev_dim, hidden_dim),\n",
    "\t\t\t\tactivation,\n",
    "\t\t\t])\n",
    "\t\t\tprev_dim = hidden_dim\n",
    "\t\tdecoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "\t\tself.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t\tself.init_weights()\n",
    "\n",
    "\tdef init_weights(self):\n",
    "\t\tdef init_layer(layer):\n",
    "\t\t\tif isinstance(layer, nn.Linear):\n",
    "\t\t\t\tnn.init.xavier_uniform_(layer.weight)\n",
    "\t\t\t\tif layer.bias is not None:\n",
    "\t\t\t\t\tnn.init.normal_(layer.bias, std=0.01)\n",
    "\n",
    "\t\tfor layer in self.encoder:\n",
    "\t\t\tinit_layer(layer)\n",
    "\t\tinit_layer(self.distribution_parameters)\n",
    "\t\tfor layer in self.decoder:\n",
    "\t\t\tinit_layer(layer)\n",
    "\n",
    "\tdef encode(self, x: torch.tensor) -> tuple[torch.tensor, torch.tensor]:\n",
    "\t\tx = F.normalize(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.encoder(x)\n",
    "\t\tdistribution_parameters = self.distribution_parameters(x)\n",
    "\t\treturn distribution_parameters[:, self.latent_dim:], distribution_parameters[:, :self.latent_dim]  # [mu, log_var]\n",
    "\n",
    "\tdef reparameterize(self, mu, log_var):\n",
    "\t\tif self.training:\n",
    "\t\t\tstd = torch.exp(0.5 * log_var)\n",
    "\t\t\teps = torch.randn_like(std)\n",
    "\t\t\treturn mu + eps * std\n",
    "\t\telse:\n",
    "\t\t\treturn mu\n",
    "\n",
    "\tdef decode(self, z: torch.tensor) -> torch.tensor:\n",
    "\t\treturn self.decoder(z)\n",
    "\n",
    "\tdef forward(self, input: torch.tensor) -> tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\t\tmu, log_var = self.encode(input)\n",
    "\t\tz = self.reparameterize(mu, log_var)\n",
    "\t\treturn self.decode(z), mu, log_var"
   ],
   "id": "39e63504730363db",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T10:46:46.579436Z",
     "start_time": "2024-11-23T10:46:46.576283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multinomial_loss(recon_x, x, mu, log_var, beta_anneal) -> torch.tensor:\n",
    "\tneg_ll = -torch.mean(torch.sum(x * F.log_softmax(recon_x, dim=1), dim=-1))\n",
    "\tkld = -0.5 * torch.mean(torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1))\n",
    "\treturn neg_ll + beta_anneal * kld"
   ],
   "id": "ed22b7288747bf11",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T10:46:46.625162Z",
     "start_time": "2024-11-23T10:46:46.622417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_anneal_rate(training_iteration, total_training_iteration, beta_cap) -> float:\n",
    "\treturn min(beta_cap, training_iteration / total_training_iteration)"
   ],
   "id": "62729bbaa6feaef",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T10:46:46.675198Z",
     "start_time": "2024-11-23T10:46:46.670580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UsersDataloader:\n",
    "\tdef __init__(self, urm: sp.csr_matrix, batch_size: int = 512, shuffle: bool = True):\n",
    "\t\tself.urm = urm\n",
    "\t\tself.num_users = self.urm.shape[0]\n",
    "\t\tself.users_idx = torch.arange(self.urm.shape[0]).long()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.curr_batch_idx = 0\n",
    "\n",
    "\t\tself.length = self.num_users // self.batch_size + 1\n",
    "\n",
    "\t\tif shuffle:\n",
    "\t\t\tself.users_idx = self.users_idx[torch.randperm(self.num_users)]\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\tself.curr_batch_idx = 0\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __next__(self):\n",
    "\t\tif self.curr_batch_idx >= self.length:\n",
    "\t\t\traise StopIteration\n",
    "\n",
    "\t\tself.curr_batch_idx += 1\n",
    "\t\treturn torch.from_numpy(self.urm[\n",
    "\t\t\tself.users_idx[(self.curr_batch_idx - 1) * self.batch_size:self.curr_batch_idx * self.batch_size]\n",
    "\t\t].toarray())\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.length"
   ],
   "id": "2e5494d837289441",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T10:46:46.740188Z",
     "start_time": "2024-11-23T10:46:46.728954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultVAEPR(RecommenderModel):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MultVAEPR, self).__init__()\n",
    "\t\tself.vae: VAE | None = None\n",
    "\t\tself.optimizer: Optimizer | None = None\n",
    "\t\tself.beta_cap: float = 0\n",
    "\t\tself.loss_fn = None\n",
    "\t\tself.best_map: float = 0\n",
    "\n",
    "\tdef fit(self, urm: sp.csr_matrix, urm_val: sp.csr_matrix, progress_bar: bool = True, hidden_dims: list[int] = None, latent_dim: int = 64, lr: float = 1e-3, beta_cap: float = .4, dropout: float = .5, weight_decay: float = 1e-8, batch_size: int = 512, epochs: int = 50, plot_loss: bool = True, **kwargs) -> None:\n",
    "\t\tself.urm = urm\n",
    "\t\tself.beta_cap = beta_cap\n",
    "\n",
    "\t\tself.vae = VAE(\n",
    "\t\t\tinput_dim=urm.shape[1],\n",
    "\t\t\thidden_dims=hidden_dims,\n",
    "\t\t\tlatent_dim=latent_dim,\n",
    "\t\t\tdropout=dropout,\n",
    "\t\t)\n",
    "\t\tself.optimizer = Adam(self.vae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\t\tself.loss_fn = multinomial_loss\n",
    "\n",
    "\t\tdataloader = UsersDataloader(self.urm, batch_size=batch_size)\n",
    "\t\tdataloader_val = UsersDataloader(urm_val, batch_size=batch_size)\n",
    "\t\tdl_len = len(dataloader)\n",
    "\t\ttotal_training_iterations = epochs * dl_len\n",
    "\n",
    "\t\tloss_history = np.zeros((epochs * dl_len))\n",
    "\t\tloss_history_val = np.zeros((epochs + 1))\n",
    "\t\tmap_history = np.zeros((epochs + 1))\n",
    "\n",
    "\t\tvalidation_enabled = urm_val.nnz > 0\n",
    "\t\tif validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\tmap_history[0], loss_history_val[0] = self._validate(dataloader_val, urm_val)\n",
    "\n",
    "\t\titerator = (t := trange(epochs, desc=\"Training...\")) if progress_bar else range(epochs)\n",
    "\t\tfor epoch in iterator:\n",
    "\t\t\tself.vae.train()\n",
    "\t\t\tfor batch_idx, users_batch in enumerate(dataloader):\n",
    "\t\t\t\trecon_x, mu, log_var = self.vae(users_batch)\n",
    "\n",
    "\t\t\t\tanneal = get_anneal_rate(epoch * batch_size + batch_idx, total_training_iterations, self.beta_cap)\n",
    "\t\t\t\tloss = self.loss_fn(recon_x, users_batch, mu, log_var, anneal)\n",
    "\n",
    "\t\t\t\tneg_ll = -torch.mean(torch.sum(users_batch * F.log_softmax(recon_x, dim=1), dim=-1))\n",
    "\t\t\t\tkld = -0.5 * torch.mean(torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1))\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\t\tif progress_bar:\n",
    "\t\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t\tt.set_postfix({\n",
    "\t\t\t\t\t\t\t\"neg_ll\": f\"{neg_ll.item():.2f}\",\n",
    "\t\t\t\t\t\t\t\"kld\": f\"{kld.item():.2f}\",\n",
    "\t\t\t\t\t\t\t\"Batch\": f\"{(batch_idx + 1) / dl_len * 100:.2f}%\",\n",
    "\t\t\t\t\t\t\t\"Train loss\": f\"{loss.item():.5f}\",\n",
    "\t\t\t\t\t\t\t\"Val loss\": f\"{loss_history_val[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\t\"MAP@10\": f\"{map_history[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\t\"Best MAP@10\": f\"{self.best_map:.5f}\",\n",
    "\t\t\t\t\t\t})\n",
    "\t\t\tif validation_enabled:\n",
    "\t\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\t\tmap_history[epoch + 1], loss_history_val[epoch + 1] = self._validate(dataloader_val, urm_val)\n",
    "\t\t\t\tself.best_map = max(self.best_map, map_history[epoch + 1])\n",
    "\t\tif not validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()\n",
    "\t\tif plot_loss:\n",
    "\t\t\tplot_losses(epochs, loss_history, loss_history_val, len(dataloader), ('MAP@10', [x * len(dataloader) for x in range(epochs + 1)], map_history))\n",
    "\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _compute_full_urm_pred(self, batch_size: int = 4096) -> None:\n",
    "\t\tdel self.urm_pred\n",
    "\t\tself.urm_pred = torch.zeros(self.urm.shape, dtype=torch.float32)\n",
    "\t\tdataloader = UsersDataloader(self.urm, batch_size=batch_size, shuffle=False)\n",
    "\t\tfor batch_idx, users_batch in enumerate(dataloader):\n",
    "\t\t\trecon_users_batch, _, _ = self.vae(users_batch)\n",
    "\t\t\tself.urm_pred[batch_idx * batch_size:(batch_idx + 1) * batch_size] = F.softmax(recon_users_batch, dim=-1)\n",
    "\n",
    "\t\tself.urm_pred = self.urm_pred.cpu().numpy()\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _validate(self, dataloader_val, urm_val):\n",
    "\t\tself.vae.eval()\n",
    "\t\tloss = 0\n",
    "\t\tfor users_batch in dataloader_val:\n",
    "\t\t\trecon_users_batch, mu, log_var = self.vae(users_batch)\n",
    "\t\t\tloss += self.loss_fn(recon_users_batch, users_batch, mu, log_var, self.beta_cap)\n",
    "\t\treturn evaluate_model(self, urm_val, users_to_test=.2), (loss / len(dataloader_val)).item()"
   ],
   "id": "122249a4e7854e89",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T10:58:06.414264Z",
     "start_time": "2024-11-23T10:46:46.781722Z"
    }
   },
   "cell_type": "code",
   "source": "_, _ = train_model(MultVAEPR(), test_size=.2, epochs=20, hidden_dims=[1024], lr=1e-3, latent_dim=256, weight_decay=0, dropout=.5, batch_size=512, beta_cap=.2,)",
   "id": "7110ac453a0c2c9a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  10%|█         | 2/20 [09:51<1:28:45, 295.85s/it, neg_ll=323.81, kld=31.11, Batch=100.00%, Train loss=330.03296, Val loss=88.08788, MAP@10=0.01251, Best MAP@10=0.01251] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMultVAEPR\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_dims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlatent_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta_cap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/src/utils.py:112\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, at, test_size, print_eval, **kwargs)\u001B[0m\n\u001B[1;32m    109\u001B[0m urm, icm \u001B[38;5;241m=\u001B[39m open_dataset()\n\u001B[1;32m    110\u001B[0m urm_train, urm_test \u001B[38;5;241m=\u001B[39m train_test_split(urm, test_size\u001B[38;5;241m=\u001B[39mtest_size)\n\u001B[0;32m--> 112\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43murm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murm_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43micm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43micm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murm_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murm_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m map_10 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m print_eval \u001B[38;5;129;01mand\u001B[39;00m test_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[6], line 65\u001B[0m, in \u001B[0;36mMultVAEPR.fit\u001B[0;34m(self, urm, urm_val, progress_bar, hidden_dims, latent_dim, lr, beta_cap, dropout, weight_decay, batch_size, epochs, plot_loss, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \t\t\tt\u001B[38;5;241m.\u001B[39mset_postfix({\n\u001B[1;32m     56\u001B[0m \t\t\t\t\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mneg_ll\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mneg_ll\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     57\u001B[0m \t\t\t\t\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkld\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkld\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     62\u001B[0m \t\t\t\t\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest MAP@10\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_map\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     63\u001B[0m \t\t\t})\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_enabled:\n\u001B[0;32m---> 65\u001B[0m \t\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute_full_urm_pred\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \tmap_history[epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m], loss_history_val[epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate(dataloader_val, urm_val)\n\u001B[1;32m     67\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_map, map_history[epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 80\u001B[0m, in \u001B[0;36mMultVAEPR._compute_full_urm_pred\u001B[0;34m(self, batch_size)\u001B[0m\n\u001B[1;32m     78\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m UsersDataloader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murm, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, users_batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m---> 80\u001B[0m \trecon_users_batch, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvae\u001B[49m\u001B[43m(\u001B[49m\u001B[43musers_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murm_pred[batch_idx \u001B[38;5;241m*\u001B[39m batch_size:(batch_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m batch_size] \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(recon_users_batch, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murm_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murm_pred\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[2], line 70\u001B[0m, in \u001B[0;36mVAE.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[torch\u001B[38;5;241m.\u001B[39mtensor, torch\u001B[38;5;241m.\u001B[39mtensor, torch\u001B[38;5;241m.\u001B[39mtensor]:\n\u001B[0;32m---> 70\u001B[0m \tmu, log_var \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m \tz \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreparameterize(mu, log_var)\n\u001B[1;32m     72\u001B[0m \t\u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode(z), mu, log_var\n",
      "Cell \u001B[0;32mIn[2], line 54\u001B[0m, in \u001B[0;36mVAE.encode\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     52\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnormalize(x)\n\u001B[1;32m     53\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[0;32m---> 54\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m distribution_parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribution_parameters(x)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m distribution_parameters[:, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent_dim:], distribution_parameters[:, :\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent_dim]\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
