{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T13:01:05.322347Z",
     "start_time": "2024-11-21T13:01:05.319098Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import trange\n",
    "\n",
    "from src.recommender_model import RecommenderModel\n",
    "from src.utils import plot_losses, evaluate_model, train_model, write_submission"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T13:01:05.364798Z",
     "start_time": "2024-11-21T13:01:05.353624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class URMDataLoaderWithNegativeSampling:\n",
    "\t\"\"\"Custom URM Dataset with Negative Sampling.\"\"\"\n",
    "\tdef __init__(self, urm: sp.csr_matrix, batch_size: int, num_neg_samples: int = 5):\n",
    "\t\tself.urm = urm\n",
    "\t\tself.num_items = self.urm.shape[1]\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.num_neg_samples = num_neg_samples\n",
    "\n",
    "\t\turm_coo = self.urm.tocoo()\n",
    "\t\tself.user_item_coordinates = np.vstack((urm_coo.row, urm_coo.col)).T\n",
    "\t\tself.user_item_sets = {\n",
    "\t\t\tuser_id: set(self.urm.getrow(user_id).indices)\n",
    "\t\t\tfor user_id in range(self.urm.shape[0])\n",
    "\t\t}\n",
    "\n",
    "\t\tself.curr_batch_idx = 0\n",
    "\t\tself.length = int(np.ceil(len(self.user_item_coordinates) / self.batch_size))\n",
    "\n",
    "\t\tself.shuffle()\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"Number of batches.\"\"\"\n",
    "\t\treturn self.length\n",
    "\n",
    "\tdef __next__(self):\n",
    "\t\t\"\"\"Fetches a whole batch.\"\"\"\n",
    "\t\tif self.curr_batch_idx >= self.length:\n",
    "\t\t\traise StopIteration\n",
    "\n",
    "\t\tstart_idx = self.curr_batch_idx * self.batch_size\n",
    "\t\tend_idx = min(start_idx + self.batch_size, len(self.user_item_coordinates))\n",
    "\n",
    "\t\tbatch_data = self.user_item_coordinates[start_idx:end_idx]\n",
    "\t\tusers = batch_data[:, 0]\n",
    "\t\tpos_samples = batch_data[:, 1]\n",
    "\n",
    "\t\tneg_samples = np.zeros((end_idx - start_idx, self.num_neg_samples), dtype=np.int64)\n",
    "\t\tfor sample_idx, user in enumerate(users):\n",
    "\t\t\tfor neg_sample_idx in range(self.num_neg_samples):\n",
    "\t\t\t\twhile True:\n",
    "\t\t\t\t\tneg_sample = np.random.randint(self.num_items)\n",
    "\t\t\t\t\tif neg_sample not in self.user_item_sets[user]:\n",
    "\t\t\t\t\t\tneg_samples[sample_idx, neg_sample_idx] = neg_sample\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\tself.curr_batch_idx += 1\n",
    "\t\treturn torch.from_numpy(users), torch.from_numpy(pos_samples).unsqueeze(-1), torch.from_numpy(neg_samples)\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\tself.curr_batch_idx = 0\n",
    "\t\treturn self\n",
    "\n",
    "\tdef shuffle(self):\n",
    "\t\t\"\"\"Shuffle the dataset.\"\"\"\n",
    "\t\tnp.random.shuffle(self.user_item_coordinates)"
   ],
   "id": "329397186635e6d7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T13:01:05.400698Z",
     "start_time": "2024-11-21T13:01:05.397628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bpr_loss(pos_scores, neg_scores, lambda_reg, embeddings_weights, nodes_idxs):\n",
    "\treturn -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores))) + lambda_reg * embeddings_weights[nodes_idxs].norm(2).pow(2)"
   ],
   "id": "d108d420f47b5a1d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T13:01:05.451767Z",
     "start_time": "2024-11-21T13:01:05.446890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LightGCNModel(nn.Module):\n",
    "\tdef __init__(self, num_users: int, num_items: int, num_layers: int = 3, embedding_dim: int = 64):\n",
    "\t\tsuper(LightGCNModel, self).__init__()\n",
    "\n",
    "\t\tself.num_users = num_users\n",
    "\t\tself.num_items = num_items\n",
    "\n",
    "\t\t# The first num_users embeddings are the user's and the next num_items are the item's\n",
    "\t\tself.embeddings = nn.Embedding(num_embeddings=self.num_users + self.num_items, embedding_dim=embedding_dim)\n",
    "\t\tnn.init.normal_(self.embeddings.weight, std=0.1)\n",
    "\n",
    "\t\tself.aggregation_layer = nn.Parameter(torch.ones(num_layers + 1) / (num_layers + 1))  # aggregation of layers outputs\n",
    "\n",
    "\t\tself.num_layers = num_layers\n",
    "\n",
    "\tdef forward(self, edge_index: SparseTensor) -> tuple[torch.tensor, torch.tensor]:\n",
    "\t\tx = self.embeddings.weight\n",
    "\n",
    "\t\tlayers_output = [x]\n",
    "\t\tfor _ in range(self.num_layers):\n",
    "\t\t\tx = edge_index @ x\n",
    "\t\t\tlayers_output.append(x)\n",
    "\n",
    "\t\tfinal_embeddings = (torch.stack(layers_output, dim=1) * F.softmax(self.aggregation_layer, dim=0).view(-1, 1)).sum(dim=1)\n",
    "\n",
    "\t\treturn final_embeddings[:self.num_users], final_embeddings[self.num_users:]"
   ],
   "id": "1d6c00fd82d9d02",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T13:01:30.016597Z",
     "start_time": "2024-11-21T13:01:29.998624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LightGCN(RecommenderModel):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(LightGCN, self).__init__()\n",
    "\t\tself.model: nn.Module | None = None\n",
    "\t\tself.optimizer: Optimizer | None = None\n",
    "\t\tself.loss_fn = None\n",
    "\t\tself.num_neg_samples: int = 0\n",
    "\t\tself.lambda_reg: float = 0\n",
    "\t\tself.edge_index: SparseTensor | None = None\n",
    "\t\tself.edge_index_norm: SparseTensor | None = None\n",
    "\t\tself.best_map = 0.0\n",
    "\n",
    "\tdef fit(self, urm: sp.csr_matrix, icm: sp.csr_matrix, urm_val: sp.csr_matrix, progress_bar: bool = True, lr: float = .001, embedding_dim: int = 32, num_layers: int = 3, epochs: int = 10, batch_size: int = 2**14, lambda_reg: float = 1e-4, weight_decay: float = 1e-8, num_neg_samples: int = 5, plot_loss: bool = True)-> None:\n",
    "\t\tself.urm = urm\n",
    "\t\tnum_users, num_items = self.urm.shape\n",
    "\t\tnum_nodes = num_users + num_items\n",
    "\t\tself.model = LightGCNModel(num_users, num_items, num_layers=num_layers, embedding_dim=embedding_dim)\n",
    "\t\tself.optimizer = Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\t\tself.loss_fn = bpr_loss\n",
    "\t\tself.num_neg_samples = num_neg_samples\n",
    "\t\tself.lambda_reg = lambda_reg\n",
    "\n",
    "\t\tvalidation_enabled = urm_val.nnz > 0\n",
    "\n",
    "\t\tprint(\"Building the datasets...\")\n",
    "\n",
    "\t\turm_coo = self.urm.tocoo()\n",
    "\t\tusers_nodes, items_nodes = torch.from_numpy(urm_coo.row).long(), torch.from_numpy(urm_coo.col + num_users).long()\n",
    "\t\tself.edge_index = SparseTensor(\n",
    "\t\t\trow=torch.cat((users_nodes, items_nodes), dim=0),\n",
    "\t\t\tcol=torch.cat((items_nodes, users_nodes), dim=0),\n",
    "\t\t\tsparse_sizes=(num_nodes, num_nodes),\n",
    "\t\t)  # we concatenate the tensors to make the graph undirected\n",
    "\n",
    "\t\tself.edge_index_norm = gcn_norm(self.edge_index, add_self_loops=False)\n",
    "\n",
    "\t\tdataloader = URMDataLoaderWithNegativeSampling(urm, batch_size=batch_size, num_neg_samples=num_neg_samples)\n",
    "\t\tdataloader_val = URMDataLoaderWithNegativeSampling(urm_val, batch_size=batch_size, num_neg_samples=num_neg_samples)\n",
    "\t\tdl_len = len(dataloader)\n",
    "\n",
    "\t\tloss_history_val = np.zeros(epochs + 1)\n",
    "\t\tmap_history = np.zeros(epochs + 1)\n",
    "\t\tloss_history = np.zeros((dl_len * epochs,))\n",
    "\n",
    "\t\tif validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\tmap_history[0], loss_history_val[0] = self._validate(dataloader_val, urm_val)\n",
    "\n",
    "\t\tprint(\"Training the model...\")\n",
    "\t\tfor epoch in (t := trange(epochs)):\n",
    "\t\t\tself.model.train()\n",
    "\t\t\tfor batch_idx, (users, pos_samples, neg_samples) in enumerate(dataloader):\n",
    "\t\t\t\tusers_embeddings, items_embeddings = self.model(self.edge_index_norm)\n",
    "\n",
    "\t\t\t\tbatch_users_embeddings = users_embeddings[users].unsqueeze(1)\n",
    "\t\t\t\tpos_items_embeddings = items_embeddings[pos_samples]\n",
    "\t\t\t\tneg_items_embeddings = items_embeddings[neg_samples]\n",
    "\n",
    "\t\t\t\tpos_scores = (batch_users_embeddings * pos_items_embeddings).sum(dim=-1)\n",
    "\t\t\t\tneg_scores = (batch_users_embeddings * neg_items_embeddings).sum(dim=-1)\n",
    "\n",
    "\t\t\t\tloss = self.loss_fn(\n",
    "\t\t\t\t\tpos_scores,\n",
    "\t\t\t\t\tneg_scores,\n",
    "\t\t\t\t\tself.lambda_reg,\n",
    "\t\t\t\t\tself.model.embeddings.weight,\n",
    "\t\t\t\t\ttorch.cat((pos_samples.view(-1), neg_samples.view(-1)))\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\t\tloss_history[dl_len * epoch + batch_idx] = loss.item()\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tt.set_postfix({\n",
    "\t\t\t\t\t\t\"Batch\": f\"{(batch_idx + 1) / dl_len * 100:.2f}%\",\n",
    "\t\t\t\t\t\t\"Train loss\": f\"{loss.item():.5f}\",\n",
    "\t\t\t\t\t\t\"Val loss\": f\"{loss_history_val[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"MAP@10\": f\"{map_history[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"Best MAP@10\": f\"{self.best_map:.5f}\",\n",
    "\t\t\t\t\t})\n",
    "\t\t\tif validation_enabled:\n",
    "\t\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\t\tmap_history[epoch + 1], loss_history_val[epoch + 1] = self._validate(dataloader_val, urm_val)\n",
    "\t\t\t\tself.best_map = max(self.best_map, map_history[epoch + 1])\n",
    "\n",
    "\t\tif not validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()  # as it has not been done before\n",
    "\n",
    "\t\tplot_losses(epochs, loss_history, loss_history_val, len(dataloader), ('MAP@10', [x * len(dataloader) for x in range(epochs + 1)], map_history))\n",
    "\t@torch.no_grad()\n",
    "\tdef _compute_full_urm_pred(self) -> None:\n",
    "\t\t\"\"\"In-place computation of the final predicted URM matrix using the final Linear layer\"\"\"\n",
    "\t\tself.model.eval()\n",
    "\t\tdel self.urm_pred  # free memory\n",
    "\n",
    "\t\tusers_embeddings, items_embeddings = self.model(self.edge_index_norm)\n",
    "\t\tself.urm_pred = (\n",
    "\t\t\tusers_embeddings @ items_embeddings.T\n",
    "\t\t).numpy()\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _validate(self, dataloader_val: URMDataLoaderWithNegativeSampling, urm_val: sp.csr_matrix) -> tuple[float, float]:\n",
    "\t\tself.model.eval()\n",
    "\t\tloss = 0\n",
    "\t\tusers_embeddings, items_embeddings = self.model(self.edge_index_norm)\n",
    "\t\tfor batch_idx, (users, pos_samples, neg_samples) in enumerate(dataloader_val):\n",
    "\t\t\tbatch_users_embeddings = users_embeddings[users].unsqueeze(1)\n",
    "\t\t\tpos_items_embeddings = items_embeddings[pos_samples]\n",
    "\t\t\tneg_items_embeddings = items_embeddings[neg_samples]\n",
    "\n",
    "\t\t\tpos_scores = (batch_users_embeddings * pos_items_embeddings).sum(dim=-1)\n",
    "\t\t\tneg_scores = (batch_users_embeddings * neg_items_embeddings).sum(dim=-1)\n",
    "\n",
    "\t\t\tloss += self.loss_fn(\n",
    "\t\t\t\tpos_scores,\n",
    "\t\t\t\tneg_scores,\n",
    "\t\t\t\tself.lambda_reg,\n",
    "\t\t\t\tself.model.embeddings.weight,\n",
    "\t\t\t\ttorch.cat((pos_samples.view(-1), neg_samples.view(-1)))\n",
    "\t\t\t)\n",
    "\n",
    "\t\tloss /= len(dataloader_val)\n",
    "\t\treturn evaluate_model(self, urm_val, users_to_test=.2), loss.item()"
   ],
   "id": "8cc597d2ee6d777",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T13:07:26.091766Z",
     "start_time": "2024-11-21T13:01:30.570082Z"
    }
   },
   "cell_type": "code",
   "source": "lightgcn_train, _ = train_model(LightGCN(), epochs=50, embedding_dim=8, lambda_reg=1e-4, weight_decay=0, num_neg_samples=4)",
   "id": "2fff4140a8bc809a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the datasets...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [05:43<30:01, 42.90s/it, Batch=27.59%, Train loss=0.69324, Val loss=0.69327, MAP@10=0.00011, Best MAP@10=0.00031] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m lightgcn_train \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mLightGCN\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambda_reg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_neg_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/RecSysCompetition2024Polimi/src/utils.py:112\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, at, test_size, print_eval, **kwargs)\u001B[0m\n\u001B[1;32m    109\u001B[0m urm, icm \u001B[38;5;241m=\u001B[39m open_dataset()\n\u001B[1;32m    110\u001B[0m urm_train, urm_test \u001B[38;5;241m=\u001B[39m train_test_split(urm, test_size\u001B[38;5;241m=\u001B[39mtest_size)\n\u001B[0;32m--> 112\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43murm_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43micm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murm_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m print_eval \u001B[38;5;129;01mand\u001B[39;00m test_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    115\u001B[0m \t\u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMAP@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mat\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m evaluation of the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevaluate_model(model,\u001B[38;5;250m \u001B[39murm_test,\u001B[38;5;250m \u001B[39mat\u001B[38;5;241m=\u001B[39mat,\u001B[38;5;250m \u001B[39musers_to_test\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m.2\u001B[39m)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[11], line 57\u001B[0m, in \u001B[0;36mLightGCN.fit\u001B[0;34m(self, urm, icm, urm_val, lr, embedding_dim, num_layers, epochs, batch_size, lambda_reg, weight_decay, num_neg_samples, plot_loss)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m (t \u001B[38;5;241m:=\u001B[39m trange(epochs)):\n\u001B[1;32m     56\u001B[0m \t\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 57\u001B[0m \u001B[43m\t\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43musers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_samples\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m\t\t\u001B[49m\u001B[43musers_embeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitems_embeddings\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index_norm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m\t\t\u001B[49m\u001B[43mbatch_users_embeddings\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43musers_embeddings\u001B[49m\u001B[43m[\u001B[49m\u001B[43musers\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 41\u001B[0m, in \u001B[0;36mURMDataLoaderWithNegativeSampling.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m neg_sample_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_neg_samples):\n\u001B[1;32m     40\u001B[0m \t\u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 41\u001B[0m \t\tneg_sample \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_items\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \t\t\u001B[38;5;28;01mif\u001B[39;00m neg_sample \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_item_sets[user]:\n\u001B[1;32m     43\u001B[0m \t\t\tneg_samples[sample_idx, neg_sample_idx] \u001B[38;5;241m=\u001B[39m neg_sample\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lightgcn_submission, _ = train_model(LightGCN(), test_size=0, epochs=20, embedding_dim=32)\n",
    "write_submission(lightgcn_submission, \"lightgcn_submission.csv\")"
   ],
   "id": "48b1a6b75b2d752c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Submission result: `0.0xxxx`",
   "id": "bee2c66f61ceb2bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
