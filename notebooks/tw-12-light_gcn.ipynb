{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T02:47:23.719715Z",
     "start_time": "2024-11-21T02:47:19.677834Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import trange\n",
    "\n",
    "from src.recommender_model import RecommenderModel\n",
    "from src.utils import plot_losses, evaluate_model, train_model, write_submission"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:47:23.730861Z",
     "start_time": "2024-11-21T02:47:23.724649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class URMDataLoaderWithNegativeSampling:\n",
    "\t\"\"\"Custom URM Dataset with Negative Sampling.\"\"\"\n",
    "\tdef __init__(self, urm: sp.csr_matrix, batch_size: int, num_neg_samples: int = 5):\n",
    "\t\tself.urm = urm\n",
    "\t\tself.num_items = self.urm.shape[1]\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.num_neg_samples = num_neg_samples\n",
    "\n",
    "\t\turm_coo = self.urm.tocoo()\n",
    "\t\tself.user_item_coordinates = np.vstack((urm_coo.row, urm_coo.col)).T\n",
    "\t\tself.user_item_sets = {\n",
    "\t\t\tuser_id: set(self.urm.getrow(user_id).indices)\n",
    "\t\t\tfor user_id in range(self.urm.shape[0])\n",
    "\t\t}\n",
    "\n",
    "\t\tself.curr_batch_idx = 0\n",
    "\t\tself.length = int(np.ceil(len(self.user_item_coordinates) / self.batch_size))\n",
    "\n",
    "\t\tself.shuffle()\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"Number of batches.\"\"\"\n",
    "\t\treturn self.length\n",
    "\n",
    "\tdef __next__(self):\n",
    "\t\t\"\"\"Fetches a whole batch.\"\"\"\n",
    "\t\tif self.curr_batch_idx >= self.length:\n",
    "\t\t\traise StopIteration\n",
    "\n",
    "\t\tstart_idx = self.curr_batch_idx * self.batch_size\n",
    "\t\tend_idx = min(start_idx + self.batch_size, len(self.user_item_coordinates))\n",
    "\n",
    "\t\tbatch_data = self.user_item_coordinates[start_idx:end_idx]\n",
    "\t\tusers = batch_data[:, 0]\n",
    "\t\tpos_samples = batch_data[:, 1]\n",
    "\n",
    "\t\tneg_samples = np.zeros((end_idx - start_idx, self.num_neg_samples), dtype=np.int64)\n",
    "\t\tfor sample_idx, user in enumerate(users):\n",
    "\t\t\tfor neg_sample_idx in range(self.num_neg_samples):\n",
    "\t\t\t\twhile True:\n",
    "\t\t\t\t\tneg_sample = np.random.randint(self.num_items)\n",
    "\t\t\t\t\tif neg_sample not in self.user_item_sets[user]:\n",
    "\t\t\t\t\t\tneg_samples[sample_idx, neg_sample_idx] = neg_sample\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\tself.curr_batch_idx += 1\n",
    "\t\treturn torch.from_numpy(users), torch.from_numpy(pos_samples).unsqueeze(-1), torch.from_numpy(neg_samples)\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\tself.curr_batch_idx = 0\n",
    "\t\treturn self\n",
    "\n",
    "\tdef shuffle(self):\n",
    "\t\t\"\"\"Shuffle the dataset.\"\"\"\n",
    "\t\tnp.random.shuffle(self.user_item_coordinates)"
   ],
   "id": "329397186635e6d7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:47:23.815217Z",
     "start_time": "2024-11-21T02:47:23.811799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bpr_loss(pos_scores, neg_scores, lambda_reg, embeddings_weights, nodes_idxs):\n",
    "\treturn -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores))) + lambda_reg * embeddings_weights[nodes_idxs].norm(2).pow(2)"
   ],
   "id": "d108d420f47b5a1d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:47:23.864882Z",
     "start_time": "2024-11-21T02:47:23.860388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LightGCNModel(nn.Module):\n",
    "\tdef __init__(self, num_users: int, num_items: int, num_layers: int = 3, embedding_dim: int = 64):\n",
    "\t\tsuper(LightGCNModel, self).__init__()\n",
    "\n",
    "\t\tself.num_users = num_users\n",
    "\t\tself.num_items = num_items\n",
    "\n",
    "\t\t# The first num_users embeddings are the user's and the next num_items are the item's\n",
    "\t\tself.embeddings = nn.Embedding(num_embeddings=self.num_users + self.num_items, embedding_dim=embedding_dim)\n",
    "\t\tnn.init.normal_(self.embeddings.weight, std=0.1)\n",
    "\n",
    "\t\tself.aggregation_layer = nn.Parameter(torch.ones(num_layers + 1) / (num_layers + 1))  # aggregation of layers outputs\n",
    "\n",
    "\t\tself.num_layers = num_layers\n",
    "\n",
    "\tdef forward(self, edge_index: SparseTensor) -> tuple[torch.tensor, torch.tensor]:\n",
    "\t\tx = self.embeddings.weight\n",
    "\n",
    "\t\tlayers_output = [x]\n",
    "\t\tfor _ in range(self.num_layers):\n",
    "\t\t\tx = edge_index @ x\n",
    "\t\t\tlayers_output.append(x)\n",
    "\n",
    "\t\tfinal_embeddings = (torch.stack(layers_output, dim=1) * F.softmax(self.aggregation_layer, dim=0).view(-1, 1)).sum(dim=1)\n",
    "\n",
    "\t\treturn final_embeddings[:self.num_users], final_embeddings[self.num_users:]"
   ],
   "id": "1d6c00fd82d9d02",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:47:23.925890Z",
     "start_time": "2024-11-21T02:47:23.914418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LightGCN(RecommenderModel):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(LightGCN, self).__init__()\n",
    "\t\tself.model: nn.Module | None = None\n",
    "\t\tself.optimizer: Optimizer | None = None\n",
    "\t\tself.loss_fn = None\n",
    "\t\tself.num_neg_samples: int = 0\n",
    "\t\tself.lambda_reg: float = 0\n",
    "\t\tself.edge_index: SparseTensor | None = None\n",
    "\t\tself.edge_index_norm: SparseTensor | None = None\n",
    "\t\tself.best_map = 0.0\n",
    "\n",
    "\tdef fit(self, urm: sp.csr_matrix, icm: sp.csr_matrix, urm_val: sp.csr_matrix, lr: float = .001, embedding_dim: int = 32, num_layers: int = 3, epochs: int = 10, batch_size: int = 2**14, lambda_reg: float = 1e-4, weight_decay: float = 1e-8, num_neg_samples: int = 5, plot_loss: bool = True)-> None:\n",
    "\t\tself.urm = urm\n",
    "\t\tnum_users, num_items = self.urm.shape\n",
    "\t\tnum_nodes = num_users + num_items\n",
    "\t\tself.model = LightGCNModel(num_users, num_items, num_layers=num_layers, embedding_dim=embedding_dim)\n",
    "\t\tself.optimizer = Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\t\tself.loss_fn = bpr_loss\n",
    "\t\tself.num_neg_samples = num_neg_samples\n",
    "\t\tself.lambda_reg = lambda_reg\n",
    "\n",
    "\t\tvalidation_enabled = urm_val.nnz > 0\n",
    "\n",
    "\t\tprint(\"Building the datasets...\")\n",
    "\n",
    "\t\turm_coo = self.urm.tocoo()\n",
    "\t\tusers_nodes, items_nodes = torch.from_numpy(urm_coo.row).long(), torch.from_numpy(urm_coo.col + num_users).long()\n",
    "\t\tself.edge_index = SparseTensor(\n",
    "\t\t\trow=torch.cat((users_nodes, items_nodes), dim=0),\n",
    "\t\t\tcol=torch.cat((items_nodes, users_nodes), dim=0),\n",
    "\t\t\tsparse_sizes=(num_nodes, num_nodes),\n",
    "\t\t)  # we concatenate the tensors to make the graph undirected\n",
    "\n",
    "\t\tself.edge_index_norm = gcn_norm(self.edge_index, add_self_loops=False)\n",
    "\n",
    "\t\tdataloader = URMDataLoaderWithNegativeSampling(urm, batch_size=batch_size, num_neg_samples=num_neg_samples)\n",
    "\t\tdataloader_val = URMDataLoaderWithNegativeSampling(urm_val, batch_size=batch_size, num_neg_samples=num_neg_samples)\n",
    "\t\tdl_len = len(dataloader)\n",
    "\n",
    "\t\tloss_history_val = np.zeros(epochs + 1)\n",
    "\t\tmap_history = np.zeros(epochs + 1)\n",
    "\t\tloss_history = np.zeros((dl_len * epochs,))\n",
    "\n",
    "\t\tif validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\tmap_history[0], loss_history_val[0] = self._validate(dataloader_val, urm_val)\n",
    "\n",
    "\t\tprint(\"Training the model...\")\n",
    "\t\tfor epoch in (t := trange(epochs)):\n",
    "\t\t\tself.model.train()\n",
    "\t\t\tfor batch_idx, (users, pos_samples, neg_samples) in enumerate(dataloader):\n",
    "\t\t\t\tusers_embeddings, items_embeddings = self.model(self.edge_index_norm)\n",
    "\n",
    "\t\t\t\tbatch_users_embeddings = users_embeddings[users].unsqueeze(1)\n",
    "\t\t\t\tpos_items_embeddings = items_embeddings[pos_samples]\n",
    "\t\t\t\tneg_items_embeddings = items_embeddings[neg_samples]\n",
    "\n",
    "\t\t\t\tpos_scores = (batch_users_embeddings * pos_items_embeddings).sum(dim=-1)\n",
    "\t\t\t\tneg_scores = (batch_users_embeddings * neg_items_embeddings).sum(dim=-1)\n",
    "\n",
    "\t\t\t\tloss = self.loss_fn(\n",
    "\t\t\t\t\tpos_scores,\n",
    "\t\t\t\t\tneg_scores,\n",
    "\t\t\t\t\tself.lambda_reg,\n",
    "\t\t\t\t\tself.model.embeddings.weight,\n",
    "\t\t\t\t\ttorch.cat((pos_samples.view(-1), neg_samples.view(-1)))\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\t\tloss_history[dl_len * epoch + batch_idx] = loss.item()\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tt.set_postfix({\n",
    "\t\t\t\t\t\t\"Batch\": f\"{(batch_idx + 1) / dl_len * 100:.2f}%\",\n",
    "\t\t\t\t\t\t\"Train loss\": f\"{loss.item():.5f}\",\n",
    "\t\t\t\t\t\t\"Val loss\": f\"{loss_history_val[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"MAP@10\": f\"{map_history[epoch]:.5f}\",\n",
    "\t\t\t\t\t\t\"Best MAP@10\": f\"{self.best_map:.5f}\",\n",
    "\t\t\t\t\t})\n",
    "\t\t\tif validation_enabled:\n",
    "\t\t\t\tself._compute_full_urm_pred()\n",
    "\t\t\t\tmap_history[epoch + 1], loss_history_val[epoch + 1] = self._validate(dataloader_val, urm_val)\n",
    "\t\t\t\tself.best_map = max(self.best_map, map_history[epoch + 1])\n",
    "\n",
    "\t\tif not validation_enabled:\n",
    "\t\t\tself._compute_full_urm_pred()  # as it has not been done before\n",
    "\n",
    "\t\tplot_losses(epochs, loss_history, loss_history_val, len(dataloader), ('MAP@10', [x * len(dataloader) for x in range(epochs + 1)], map_history))\n",
    "\t@torch.no_grad()\n",
    "\tdef _compute_full_urm_pred(self) -> None:\n",
    "\t\t\"\"\"In-place computation of the final predicted URM matrix using the final Linear layer\"\"\"\n",
    "\t\tself.model.eval()\n",
    "\t\tdel self.urm_pred  # free memory\n",
    "\n",
    "\t\tusers_embeddings, items_embeddings = self.model(self.edge_index_norm)\n",
    "\t\tself.urm_pred = (\n",
    "\t\t\tusers_embeddings @ items_embeddings.T\n",
    "\t\t).numpy()\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef _validate(self, dataloader_val: URMDataLoaderWithNegativeSampling, urm_val: sp.csr_matrix) -> tuple[float, float]:\n",
    "\t\tself.model.eval()\n",
    "\t\tloss = 0\n",
    "\t\tusers_embeddings, items_embeddings = self.model(self.edge_index_norm)\n",
    "\t\tfor batch_idx, (users, pos_samples, neg_samples) in enumerate(dataloader_val):\n",
    "\t\t\tbatch_users_embeddings = users_embeddings[users].unsqueeze(1)\n",
    "\t\t\tpos_items_embeddings = items_embeddings[pos_samples]\n",
    "\t\t\tneg_items_embeddings = items_embeddings[neg_samples]\n",
    "\n",
    "\t\t\tpos_scores = (batch_users_embeddings * pos_items_embeddings).sum(dim=-1)\n",
    "\t\t\tneg_scores = (batch_users_embeddings * neg_items_embeddings).sum(dim=-1)\n",
    "\n",
    "\t\t\tloss += self.loss_fn(\n",
    "\t\t\t\tpos_scores,\n",
    "\t\t\t\tneg_scores,\n",
    "\t\t\t\tself.lambda_reg,\n",
    "\t\t\t\tself.model.embeddings.weight,\n",
    "\t\t\t\ttorch.cat((pos_samples.view(-1), neg_samples.view(-1)))\n",
    "\t\t\t)\n",
    "\n",
    "\t\tloss /= len(dataloader_val)\n",
    "\t\treturn evaluate_model(self, urm_val, users_to_test=.2), loss.item()"
   ],
   "id": "8cc597d2ee6d777",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:55:04.389857Z",
     "start_time": "2024-11-21T02:47:23.987033Z"
    }
   },
   "cell_type": "code",
   "source": "lightgcn_train = train_model(LightGCN(), epochs=20, embedding_dim=16, lambda_reg=1e-5, weight_decay=0, num_neg_samples=4)",
   "id": "2fff4140a8bc809a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the datasets...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [07:26<09:05, 49.63s/it, Batch=14.94%, Train loss=0.39876, Val loss=0.41184, MAP@10=0.00575, Best MAP@10=0.00616]  \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lightgcn_submission = train_model(LightGCN(), test_size=0, epochs=20, embedding_dim=32)\n",
    "write_submission(lightgcn_submission, \"lightgcn_submission.csv\")"
   ],
   "id": "48b1a6b75b2d752c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Submission result: `0.0xxxx`",
   "id": "bee2c66f61ceb2bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
